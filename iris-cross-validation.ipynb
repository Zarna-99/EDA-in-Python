{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#477482  ;\n           font-size:400%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 20px;\n              color:white;\n               text-align:center\">\n    <b>\n        Cross-Validation\n        </b>\n</p>\n</div></h1>","metadata":{"id":"9FKEtwUGon-H"}},{"cell_type":"markdown","source":"<img src='https://www.mihaileric.com/static/model-selection-meme-bd4a6a86f615583d1a1bbc497ca4640e-67414.jpeg' width=\"900\" height=\"600\" align=\"middle\">","metadata":{}},{"cell_type":"markdown","source":"# **\"Deploying the model without validating it on a test dataset, would be the greatest act of stupidity, in the process of machine learning.\"**","metadata":{}},{"cell_type":"markdown","source":"### Table of Contents\n\n* [1. Introduction](#1)\n* [2. Importing necessary Libraries](#2)\n* [3. Importing the Dataset](#3)\n* [4. Basic information about the dataset](#4)\n* [5. Label Encoding](#5)\n* [6. Defining the explanatory variables and target variables](#6)\n* [7. Creating the model](#7)\n* [8. Cross Validation](#8)\n     * [8.1 Method 1 : Holding out cross validation](#8.1)\n     * [8.2 Method 2 : K-Folds cross validation](#8.2)\n     * [8.3 Method 3 : Leave One Out cross validation](#8.3)","metadata":{}},{"cell_type":"markdown","source":"<h2><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#749FAD  ;\n           font-size:300%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n              color:white;\n               text-align:center\">\n    <b>\n        Introduction<a class=\"anchor\" id=\"1\"></a>\n        </b>\n</p>\n</div></h2>\n","metadata":{}},{"cell_type":"markdown","source":"In machine learning, there is always the need to test the stability of the model. It means based only on the training dataset; we can't fit our model and deploy it for further analysis. \nYou need some kind of assurance that your model has got most of the patterns from the data correct, and its not picking up too much on the noise, or in other words its low on bias and variance.\nFor this purpose, we reserve a particular sample of the dataset, which was not part of the training dataset. After that, we test our model on that sample before deployment, and this complete process comes under cross-validation.\nIn this notebook we shall see three methods for cross validation.\n*   Holding out cross validation\n*   k-folds cross validation\n*   Leave one out cross validation\n","metadata":{"id":"krgJy0xVmLrv"}},{"cell_type":"markdown","source":"<h2><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#749FAD  ;\n           font-size:300%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n              color:white;\n               text-align:center\">\n    <b>\n        Importing necessary libraries<a class=\"anchor\" id=\"2\"></a>\n        </b>\n</p>\n</div></h2>\n","metadata":{"id":"qFOTx6bwos6v"}},{"cell_type":"code","source":"#importing the necessary libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, cross_val_score, LeaveOneOut\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import tree, preprocessing\nfrom sklearn.metrics import classification_report, accuracy_score","metadata":{"id":"if2x4JQXJKSn","execution":{"iopub.status.busy":"2022-09-21T18:05:40.725595Z","iopub.execute_input":"2022-09-21T18:05:40.726022Z","iopub.status.idle":"2022-09-21T18:05:40.732066Z","shell.execute_reply.started":"2022-09-21T18:05:40.725985Z","shell.execute_reply":"2022-09-21T18:05:40.730779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#749FAD  ;\n           font-size:300%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n              color:white;\n               text-align:center\">\n    <b>\n        Importing the dataset<a class=\"anchor\" id=\"3\"></a>\n        </b>\n</p>\n</div></h2>\n","metadata":{"id":"Yam2mbjXo0ps"}},{"cell_type":"markdown","source":"\n*   The data set consists of 50 samples from each of three species of Iris (Iris Setosa, Iris virginica, and Iris versicolor).\n\n*   Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.","metadata":{"id":"4OgYh2GhpQ88"}},{"cell_type":"code","source":"data = pd.read_csv(\"../input/iris-flower-dataset/IRIS.csv\")\ndata","metadata":{"id":"uTQkj3qVLDaV","outputId":"0438e785-3e26-4805-d87a-9a726705f956","execution":{"iopub.status.busy":"2022-09-21T18:05:40.734589Z","iopub.execute_input":"2022-09-21T18:05:40.735815Z","iopub.status.idle":"2022-09-21T18:05:40.788188Z","shell.execute_reply.started":"2022-09-21T18:05:40.735768Z","shell.execute_reply":"2022-09-21T18:05:40.786748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#749FAD  ;\n           font-size:300%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n              color:white;\n               text-align:center\">\n    <b>\n        Getting the basic information of the data<a class=\"anchor\" id=\"4\"></a>\n        </b>\n</p>\n</div></h2>","metadata":{"id":"Eii_ZnbzpmDC"}},{"cell_type":"code","source":"data.info()","metadata":{"id":"lavc_UVCgESm","outputId":"2a45edf3-3e6d-4c59-b3d0-2d753a8af411","execution":{"iopub.status.busy":"2022-09-21T18:05:40.789351Z","iopub.execute_input":"2022-09-21T18:05:40.789670Z","iopub.status.idle":"2022-09-21T18:05:40.818402Z","shell.execute_reply.started":"2022-09-21T18:05:40.789641Z","shell.execute_reply":"2022-09-21T18:05:40.817022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Seeing at the basic information, there are no null values in the datset.","metadata":{"id":"fAXOK3aeptSS"}},{"cell_type":"code","source":"data[\"species\"].value_counts()","metadata":{"id":"F3NF74UqdMCU","outputId":"f58b83cc-30dd-4014-dd49-502298a4c8aa","execution":{"iopub.status.busy":"2022-09-21T18:05:40.820042Z","iopub.execute_input":"2022-09-21T18:05:40.820600Z","iopub.status.idle":"2022-09-21T18:05:40.828587Z","shell.execute_reply.started":"2022-09-21T18:05:40.820565Z","shell.execute_reply":"2022-09-21T18:05:40.827376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The dataset has equal number of data points for all the three categories, hence it is a balanced dataset.","metadata":{"id":"2lYyMr_nqBVs"}},{"cell_type":"markdown","source":"<h2><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#749FAD  ;\n           font-size:300%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n              color:white;\n               text-align:center\">\n    <b>\n        Converting the species into labels<a class=\"anchor\" id=\"5\"></a>\n        </b>\n</p>\n</div></h2>","metadata":{"id":"JDx3WZ9wqRMa"}},{"cell_type":"markdown","source":"For building any models, it is necessary to not have any string values in the dataset. Hence, we shall convert the species into labels. The labels are created as follows:\n*   0 : setosa\n*   1 : versicolor\n*   2 : virginica\n\n","metadata":{"id":"SKAJjc2RqXIw"}},{"cell_type":"code","source":"#converting into labels\nlabel_encoder = preprocessing.LabelEncoder()\ndata['species']= label_encoder.fit_transform(data['species'])\ndata[\"species\"].value_counts()","metadata":{"id":"LNB9-Y8PdUbz","outputId":"2aaa1120-1b79-45fe-c37c-9a95d7999b87","execution":{"iopub.status.busy":"2022-09-21T18:05:40.832294Z","iopub.execute_input":"2022-09-21T18:05:40.832959Z","iopub.status.idle":"2022-09-21T18:05:40.844083Z","shell.execute_reply.started":"2022-09-21T18:05:40.832913Z","shell.execute_reply":"2022-09-21T18:05:40.843047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#749FAD  ;\n           font-size:300%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n              color:white;\n               text-align:center\">\n    <b>\n        Defining the explanatory variables and the target variable<a class=\"anchor\" id=\"6\"></a>\n        </b>\n</p>\n</div></h2>","metadata":{"id":"gg-1dPZi2n9P"}},{"cell_type":"code","source":"#defining x and y variables\nx=data.iloc[:,0:4]\ny=data['species']","metadata":{"id":"eXz2DAexdj2Q","execution":{"iopub.status.busy":"2022-09-21T18:05:40.845586Z","iopub.execute_input":"2022-09-21T18:05:40.846115Z","iopub.status.idle":"2022-09-21T18:05:40.856145Z","shell.execute_reply.started":"2022-09-21T18:05:40.846083Z","shell.execute_reply":"2022-09-21T18:05:40.855198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#749FAD  ;\n           font-size:300%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n              color:white;\n               text-align:center\">\n    <b>\n        Creating the model<a class=\"anchor\" id=\"7\"></a>\n        </b>\n</p>\n</div></h2>","metadata":{"id":"zxMxrTgk25bN"}},{"cell_type":"code","source":"model = DecisionTreeClassifier(criterion = 'gini', max_depth=3)","metadata":{"id":"kNdO4XOpg0Ob","execution":{"iopub.status.busy":"2022-09-21T18:05:40.857430Z","iopub.execute_input":"2022-09-21T18:05:40.857937Z","iopub.status.idle":"2022-09-21T18:05:40.870179Z","shell.execute_reply.started":"2022-09-21T18:05:40.857905Z","shell.execute_reply":"2022-09-21T18:05:40.868905Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#749FAD  ;\n           font-size:300%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n              color:white;\n               text-align:center\">\n    <b>\n        Cross Validation<a class=\"anchor\" id=\"8\"></a>\n        </b>\n</p>\n</div></h2>\n\n","metadata":{"id":"DBkrHgOi2-Mt"}},{"cell_type":"markdown","source":"Upto now, we have created the decision tree model. Now its time to train the model on the dataset. At this point we need to make sure that the model is trained properly, such that it is able to study all the patterns in the data.For this purpose we shall apply cross validation.","metadata":{"id":"hc06oQDy3ExI"}},{"cell_type":"markdown","source":"<h3><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#96BFCC  ;\n           font-size:300%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n              color:white;\n               text-align:center\">\n    <b>\n        Method 1 : Holding out Cross Validation<a class=\"anchor\" id=\"8.1\"></a>\n        </b>\n</p>\n</div></h3>\n\n\n","metadata":{"id":"5hODs3y_lz_T"}},{"cell_type":"markdown","source":"Holding out is the traditional approach for cross validation, where the data is simply divided into two parts i.e. the training and the testing part. The data can be divided into 70-30 or 60-40, 75-25 or 80-20, or even 50-50 depending on the use case. During the training phase we would only show the training part of the dataset to the model. From this, the model will try to understand the patterns in the dataset. After the training phase, the model will be tested on the data points which were stored in the testing part. The predictions of those data points will then be compared with the actual values and hence we shall evaluate that how well the model is able to deal with the unseen dataset.","metadata":{"id":"1sj_Y3yY9ib2"}},{"cell_type":"markdown","source":"<img src='https://d1m75rqqgidzqn.cloudfront.net/wp-data/2020/07/15185319/blogs-15-7-2020-02-1024x565.jpg'>","metadata":{"id":"kFl09WqtBVoc"}},{"cell_type":"markdown","source":"Following is the code for holding out method:\n\n---\n\n","metadata":{"id":"97TQzHuLBju1"}},{"cell_type":"markdown","source":"Starting with splitting the dataset into traing and testing parts. For this we have used the \"train_test_split\" from the sklearn.model_selection library.","metadata":{"id":"uil07DFcBteG"}},{"cell_type":"code","source":"# Splitting data into training and testing data set\n# from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=40)","metadata":{"id":"WNcXdW6wdtsD","execution":{"iopub.status.busy":"2022-09-21T18:05:40.871686Z","iopub.execute_input":"2022-09-21T18:05:40.872296Z","iopub.status.idle":"2022-09-21T18:05:40.885200Z","shell.execute_reply.started":"2022-09-21T18:05:40.872263Z","shell.execute_reply":"2022-09-21T18:05:40.883942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"x_train: \" ,x_train.shape)\nprint(\"x_test: \" ,x_test.shape)\nprint(\"y_train: \" ,y_train.shape)\nprint(\"y_test: \" ,y_test.shape)","metadata":{"id":"9qFh9LLUjTTd","outputId":"06ccecf0-01e7-4e87-c568-49f9c780cdf5","execution":{"iopub.status.busy":"2022-09-21T18:05:40.886967Z","iopub.execute_input":"2022-09-21T18:05:40.887322Z","iopub.status.idle":"2022-09-21T18:05:40.900505Z","shell.execute_reply.started":"2022-09-21T18:05:40.887291Z","shell.execute_reply":"2022-09-21T18:05:40.898617Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The train test split divided the dataset into two categories. There were in total 150 datapoints, out of which 120 datapoints have been grouped as training and the remaining 30 are kept as testing.","metadata":{"id":"XoaGp6XUCX3g"}},{"cell_type":"markdown","source":"We shall now fit the model only on the training dataset.","metadata":{"id":"qlr76sISCz_8"}},{"cell_type":"code","source":"model.fit(x_train,y_train)","metadata":{"id":"sh2h9wVRd5bJ","outputId":"8c159358-f326-4f51-8da4-da4e7340717f","execution":{"iopub.status.busy":"2022-09-21T18:05:40.902090Z","iopub.execute_input":"2022-09-21T18:05:40.902731Z","iopub.status.idle":"2022-09-21T18:05:40.920532Z","shell.execute_reply.started":"2022-09-21T18:05:40.902686Z","shell.execute_reply":"2022-09-21T18:05:40.919387Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on the model fitted, now we have made the predictions for the test dataset.","metadata":{"id":"mmWOKwP1C9ov"}},{"cell_type":"code","source":"#Predicting on test data\npreds = model.predict(x_test) \npreds","metadata":{"id":"UO8433JXehyH","outputId":"eedd6db4-0d0a-40f8-8079-9b5fa340e01f","execution":{"iopub.status.busy":"2022-09-21T18:05:40.921831Z","iopub.execute_input":"2022-09-21T18:05:40.923123Z","iopub.status.idle":"2022-09-21T18:05:40.932174Z","shell.execute_reply.started":"2022-09-21T18:05:40.923084Z","shell.execute_reply":"2022-09-21T18:05:40.931187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Comparing the values of predictions and the actual labels.","metadata":{"id":"ePNQyJp3DNOC"}},{"cell_type":"code","source":"pd.Series(preds).value_counts() # getting the count of each category ","metadata":{"id":"LTrP2OD3e5P-","outputId":"4644b7e0-9185-4e99-ddab-c353ced65e8a","execution":{"iopub.status.busy":"2022-09-21T18:05:40.933708Z","iopub.execute_input":"2022-09-21T18:05:40.934064Z","iopub.status.idle":"2022-09-21T18:05:40.946805Z","shell.execute_reply.started":"2022-09-21T18:05:40.934033Z","shell.execute_reply":"2022-09-21T18:05:40.945541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test.value_counts()","metadata":{"id":"mvSoivdbfAQb","outputId":"9bc74548-b9bb-406e-a42e-eb8b6961c624","execution":{"iopub.status.busy":"2022-09-21T18:05:40.948259Z","iopub.execute_input":"2022-09-21T18:05:40.949240Z","iopub.status.idle":"2022-09-21T18:05:40.956389Z","shell.execute_reply.started":"2022-09-21T18:05:40.949204Z","shell.execute_reply":"2022-09-21T18:05:40.955541Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Getting the accuracy of the datset","metadata":{"id":"hJSk9LM7DVAZ"}},{"cell_type":"code","source":"accuracy_score(y_test,preds)*100","metadata":{"id":"caSAXeA1fCpz","outputId":"20f504e5-e8e3-4562-d025-0d58d4a10754","execution":{"iopub.status.busy":"2022-09-21T18:05:40.959774Z","iopub.execute_input":"2022-09-21T18:05:40.960311Z","iopub.status.idle":"2022-09-21T18:05:40.972491Z","shell.execute_reply.started":"2022-09-21T18:05:40.960273Z","shell.execute_reply":"2022-09-21T18:05:40.971270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy of the model is around 96.66%, which tells us that the model is working quite well on the unseen data points also.","metadata":{"id":"iusCNqA2DZwd"}},{"cell_type":"markdown","source":"Advantages and Drawbacks of Holding out Method:\n\n---\n\n\n*   Advantages:\n> One of the major advantages of this method is that it is computationally inexpensive compared to other cross-validation techniques.\n\n*   Drawbacks:\n> In the Hold out method, the test error rates are highly variable (high variance) and it totally depends on which observations end up in the training set and test set.Only a part of the data is used to train the model (high bias) which is not a very good idea when data is not huge and this will lead to overestimation of test error.\n","metadata":{"id":"vaIGlPuCMQXz"}},{"cell_type":"markdown","source":"<h3><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#96BFCC  ;\n           font-size:300%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n              color:white;\n               text-align:center\">\n    <b>\n        Method 2 : k-folds Cross Validation<a class=\"anchor\" id=\"8.2\"></a>\n        </b>\n</p>\n</div></h3>\n\n\n\n\n","metadata":{"id":"BfSOldo5l7hl"}},{"cell_type":"markdown","source":"As there is never enough data to train your model, removing a part of it for validation poses a problem of underfitting. By reducing the training data, we risk losing important patterns/ trends in data set, which in turn increases error induced by bias. So, what we require is a method that provides ample data for training the model and also leaves ample data for validation. K Fold cross validation does exactly that.\n\nIn this resampling technique, the whole data is divided into k sets of almost equal sizes. The first set is selected as the test set and the model is trained on the remaining k-1 sets. In the second iteration, the 2nd set is selected as a test set and the remaining k-1 sets are used to train the data. This process continues for all the k sets. At the end we are averaging the outputs of k fitted models to get the accuracy score of the model.","metadata":{"id":"ZX5MvUW8EFfy"}},{"cell_type":"markdown","source":"<img src='https://www.datasciencecentral.com/wp-content/uploads/2021/10/k-fold_cross_validation_en-1.jpg' width=\"900\" height=\"500\">\n","metadata":{"id":"yQd4YzNuGaqP"}},{"cell_type":"markdown","source":"Following is the code for k-fold method:\n\n---\n","metadata":{"id":"pzklJQUYHIvr"}},{"cell_type":"markdown","source":"For the k-fold cross validation we have the \"cross_val_score\" module from the sklearn.model_selection library.","metadata":{"id":"ipPziCZJHYLA"}},{"cell_type":"code","source":"scores = cross_val_score(model, x, y, cv=5)\nscores","metadata":{"id":"x3oGDwBsfOR_","outputId":"a4187a94-0fd7-450d-839b-a0d75d9682f5","execution":{"iopub.status.busy":"2022-09-21T18:05:40.973993Z","iopub.execute_input":"2022-09-21T18:05:40.974474Z","iopub.status.idle":"2022-09-21T18:05:41.013409Z","shell.execute_reply.started":"2022-09-21T18:05:40.974360Z","shell.execute_reply":"2022-09-21T18:05:41.012049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores.mean()*100","metadata":{"id":"fjH7zd0Uf8HM","outputId":"746b26e1-9cdf-4f17-ae7a-2b743fc31c8d","execution":{"iopub.status.busy":"2022-09-21T18:05:41.015119Z","iopub.execute_input":"2022-09-21T18:05:41.015619Z","iopub.status.idle":"2022-09-21T18:05:41.023405Z","shell.execute_reply.started":"2022-09-21T18:05:41.015572Z","shell.execute_reply":"2022-09-21T18:05:41.022367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy of the model comes out to be 97.33% which is slightly higher than the holding out method.","metadata":{"id":"KkyS5UQOHuqB"}},{"cell_type":"markdown","source":"Advantages and Drawbacks of k-fold method:\n\n---\n\n\n\n*   Advantages:\n> The best part about this method is each data point gets to be in the test set exactly once and gets to be part of the training set k-1 times. As the number of folds k increases, the variance also decreases (low variance).\n\n*   Drawbacks:\n> The major disadvantage of this method is that the model has to be run from scratch k-times and is computationally expensive than the Hold Out method.\n\n","metadata":{"id":"GG6h3CZxNr3X"}},{"cell_type":"markdown","source":"<h3><div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#96BFCC  ;\n           font-size:300%;\n           font-family:Sans-serif;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 15px;\n              color:white;\n               text-align:center\">\n    <b>\n        Method 3 : Leave one out Cross Validation<a class=\"anchor\" id=\"8.3\"></a>\n        </b>\n</p>\n</div></h3>\n\n\n","metadata":{"id":"sWss4NCjmD2V"}},{"cell_type":"markdown","source":"LOOCV(Leave One Out Cross-Validation) is a type of cross-validation approach in which each observation is considered as the validation set and the rest (N-1) observations are considered as the training set. In LOOCV, fitting of the model is done and predicting using one observation validation set. This process continues ‘N’ times and the average of all these iterations is calculated. This is a special case of K-fold cross-validation in which the number of folds is the same as the number of observations(K = N).","metadata":{"id":"RJZ2pI-AIGH2"}},{"cell_type":"markdown","source":"<img src='https://d2mk45aasx86xg.cloudfront.net/image2_11zon_cac3fb4270.webp'>","metadata":{"id":"hhawz4glJTjX"}},{"cell_type":"markdown","source":"Following is the code for Leave one out method:\n\n---","metadata":{"id":"SwlvCFahJ07i"}},{"cell_type":"markdown","source":"For the leave one out cross validation we have the \"LeaveOneOut\" module from the sklearn.model_selection library. The code will remain same as that of k-fold, but instead of specifying the number of splits for the \"cv\" parameter, we will pass the \"LeaveOneOut()\" function.","metadata":{"id":"lcF_zvIuJ6T7"}},{"cell_type":"code","source":"scores1 = cross_val_score(model, x, y, cv=LeaveOneOut())\nprint(scores1)","metadata":{"id":"yBi4C-qDf0yN","outputId":"703b2131-b519-4064-b83d-fb0768337a9d","execution":{"iopub.status.busy":"2022-09-21T18:05:41.024647Z","iopub.execute_input":"2022-09-21T18:05:41.025837Z","iopub.status.idle":"2022-09-21T18:05:41.661771Z","shell.execute_reply.started":"2022-09-21T18:05:41.025804Z","shell.execute_reply":"2022-09-21T18:05:41.660434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores1.mean()*100","metadata":{"id":"QUiMqva-ln7-","outputId":"516a3c22-f851-468c-cb22-f6d2172959ea","execution":{"iopub.status.busy":"2022-09-21T18:05:41.663540Z","iopub.execute_input":"2022-09-21T18:05:41.664040Z","iopub.status.idle":"2022-09-21T18:05:41.672159Z","shell.execute_reply.started":"2022-09-21T18:05:41.663993Z","shell.execute_reply":"2022-09-21T18:05:41.670864Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The accuracy of the model comes out to be 94.67% which is quite good but is less than that of previous methods.","metadata":{"id":"_k82xjx4LipK"}},{"cell_type":"markdown","source":"Advantages and Drawbacks of Leave one Out method:\n\n---\n\n\n\n*   Advantages:\n> This method helps to reduce Bias and Randomness. \n\n*   Drawbacks:\n> LOOCV has an extremely high variance because we are averaging the output of n-models which are fitted on an almost identical set of observations, and their outputs are highly positively correlated with each other.Also this is computationally expensive as the model is run ‘n’ times to test every observation in the data. ","metadata":{"id":"m_YgwTIyOiMN"}}]}